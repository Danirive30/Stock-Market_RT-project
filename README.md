# Stock Market Kafka Real-Time Data Engineering Project

## ğŸ“˜ IntroducciÃ³n
Este proyecto implementa un pipeline de datos en tiempo real para datos del mercado de valores usando Kafka, con un enfoque en la ingenierÃ­a de datos de extremo a extremo. A travÃ©s de este proyecto, exploramos el uso de diversas tecnologÃ­as y servicios de Amazon Web Services (AWS) para construir una arquitectura de datos en tiempo real que permite procesar, almacenar y consultar datos de manera eficiente.

## ğŸ— Arquitectura
Este proyecto utiliza una arquitectura de datos basada en Kafka para la ingesta de datos en tiempo real, que posteriormente se almacena en S3 y se consulta con Athena y Glue. La arquitectura consta de las siguientes etapas:
1. **Ingesta de datos** en tiempo real con Apache Kafka.
2. **Almacenamiento de datos** en AWS S3.
3. **CatalogaciÃ³n y descubrimiento** de datos usando Glue Crawler y Glue Catalog.
4. **Consulta de datos** con AWS Athena.
5. **OrquestaciÃ³n y monitoreo** en una instancia de EC2 en AWS.

## ğŸ› ï¸ TecnologÃ­as Utilizadas
Este proyecto utiliza las siguientes tecnologÃ­as:
- **Lenguaje de programaciÃ³n**: Python
- **Amazon Web Services (AWS)**:
  - **S3**: para el almacenamiento de datos
  - **Athena**: para consultas y anÃ¡lisis de datos
  - **Glue Crawler y Glue Catalog**: para catalogar y descubrir datos en S3
  - **EC2**: para el entorno de orquestaciÃ³n y monitoreo
- **Apache Kafka**: para la ingesta de datos en tiempo real

## ğŸ“Š Dataset Utilizado
Para este proyecto, se puede utilizar cualquier dataset de mercado de valores, ya que el foco estÃ¡ en la construcciÃ³n de un pipeline de datos en tiempo real. AquÃ­ estÃ¡ el dataset usado como referencia en el tutorial: [indexProcessed.csv](https://github.com/darshilparmar/stock-market-kafka-data-engineering-project/blob/main/indexProcessed.csv).

## ğŸ“º Tutorial Completo en Video
Para una guÃ­a paso a paso de la implementaciÃ³n de este proyecto, puedes ver el siguiente tutorial: [Video Tutorial](https://www.youtube.com/embed/KerNf0NANMo).

## ğŸš€ CÃ³mo Empezar
1. **Configura Apache Kafka** en una instancia de EC2 para la ingesta de datos.
2. **Carga el dataset** en Kafka para su procesamiento en tiempo real.
3. **Configura S3** en AWS para almacenar los datos.
4. **Crea un Crawler y un catÃ¡logo de datos** en Glue para la organizaciÃ³n y descubrimiento de datos.
5. **Ejecuta consultas en Athena** para analizar los datos almacenados en S3.
6. **Monitorea el pipeline** desde EC2 para asegurar la operaciÃ³n en tiempo real.

## ğŸ“„ Licencia
Este proyecto se distribuye bajo la licencia MIT. Para mÃ¡s informaciÃ³n, consulta el archivo LICENSE en este repositorio.

## ğŸ¤ Contribuciones
Â¡Las contribuciones son bienvenidas! Si tienes ideas para mejorar este proyecto, no dudes en abrir un issue o enviar un pull request.


# Stock-Market_RT-project
